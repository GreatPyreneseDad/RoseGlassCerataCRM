# CERATA Trials - Evolution Through Competition

**Philosophy**: "Test everything, promote winners"

Trials pit experimental approaches against current standards. Winners become the new standard, losers feed the graveyard with learnings.

## Active Trials

### 1. Crawl4AI Enhanced Hunter

**Status**: Ready to run
**File**: `crawl4ai_hunter_trial.py`

**Comparison**:
- **Classic**: Requests-based synchronous scraping
- **Experimental**: Async Crawl4AI with LLM fallback

**Metrics**:
- Speed: Pages crawled per minute
- Accuracy: Lead extraction completeness
- Stealth: Success rate avoiding detection
- Richness: Average fields per lead

**Success Criteria**:
- 15%+ speed improvement
- 10%+ accuracy improvement
- Equal or better stealth

**How to Run**:

```bash
# Prerequisites
pip install crawl4ai playwright pydantic
playwright install

# Run trial demo
python trials/crawl4ai_hunter_trial.py

# Or integrate with your hunting code:
from trials.crawl4ai_hunter_trial import CrawlAIHunterTrial

trial = CrawlAIHunterTrial()
trial_id = trial.create_trial()
trial.trial.start()

# In your hunt loop:
branch = trial.trial.assign_branch()
if branch == 'experimental':
    # Use Crawl4AI
    results = await enhanced_hunter.hunt_leads(urls, schema)
else:
    # Use classic
    results = classic_hunter.scrape(urls)

# After min_sample_size (50+ URLs per branch):
recommendation = trial.evaluate()

if recommendation == 'promote':
    trial.promote()  # Experimental becomes standard!
```

**Expected Outcome**:
```
üß™ Created Crawl4AI Hunter Trial: trial_20260117_123456
  Classic: requests_based
  Experimental: async_crawl4ai
  Split: 50/50
  Min sample: 50 URLs per branch

üîç Running hunt comparison on 3 URLs...

‚ñ∂Ô∏è  EXPERIMENTAL: Crawl4AI Enhanced Hunter
  ‚úì Experimental complete:
    Duration: 5.23s
    Leads found: 47
    Success rate: 100.0%
    Avg coherence: 0.82
    Speed: 34.5 pages/min

‚ñ∂Ô∏è  CLASSIC: Traditional Web Hunter
  ‚úì Classic complete:
    Duration: 26.15s (simulated)
    Leads found: 42 (simulated)
    Success rate: 80.0% (simulated)
    Speed: 6.9 pages/min

üìä TRIAL EVALUATION COMPLETE
  Winner: experimental
  Confidence: 95.2%
  Improvement: 42.3%
  Recommendation: PROMOTE
  Rationale: Experimental shows 42.3% improvement (fitness: 0.892 vs 0.627)

üöÄ PROMOTED: Crawl4AI Enhanced Hunter ‚Üí NEW STANDARD
  Classic hunter archived
  Enhanced hunter now default for web hunting
```

## Trial System Architecture

### Core Components

**TrialManager** (`src/trial/trial_manager.py`)
- Creates and manages trials
- Tracks metrics for classic vs experimental branches
- Evaluates results with statistical confidence
- Promotes winners, archives losers

**Trial**
- Represents one A/B test
- Contains two branches: classic and experimental
- Assigns traffic based on split ratio
- Tracks status: PLANNED ‚Üí RUNNING ‚Üí COMPLETED ‚Üí PROMOTED/ARCHIVED

**TrialBranch**
- One side of the comparison
- Tracks metrics: leads_qualified, outcomes, revenue, cost
- Calculates fitness score for winner determination

**TrialResult**
- Evaluation outcome
- Winner, confidence, improvement %
- Recommendation: promote, continue, or archive

### Creating a New Trial

```python
from src.trial.trial_manager import TrialManager

manager = TrialManager()

trial = manager.create_trial(
    name="Your Trial Name",
    description="What you're testing",
    experimental_config={
        # Your experimental configuration
        'feature_x': True,
        'parameter_y': 0.8,
    },
    traffic_split=0.5,  # 50/50 split
    min_sample_size=50   # Min data points before evaluation
)

trial.start()
```

### Recording Metrics

```python
# Record a qualification
manager.record_qualification(
    trial_id=trial.trial_id,
    branch='experimental',  # or 'classic'
    tier='hot'  # or 'warm', 'cold', 'disqualified'
)

# Record an outcome
manager.record_outcome(
    trial_id=trial.trial_id,
    branch='experimental',
    outcome_type='won',  # or 'lost'
    deal_value=50000,
    cost=5000
)
```

### Evaluating and Promoting

```python
# Evaluate trial (requires min_sample_size per branch)
result = manager.evaluate_trial(trial.trial_id)

print(f"Winner: {result.winner}")
print(f"Confidence: {result.confidence:.1%}")
print(f"Recommendation: {result.recommendation}")

# If experimental won:
if result.recommendation == 'promote':
    manager.promote_experimental(trial.trial_id)
    # Experimental becomes new standard!
```

## Metrics Mapping for Different Trial Types

### Lead Qualification Trials
- `leads_qualified`: Total leads processed
- `leads_hot/warm/cold`: Quality tiers
- `outcomes_won/lost`: Conversion results

### Web Hunting Trials (Crawl4AI)
- `leads_qualified`: URLs scraped
- `leads_hot`: High coherence extractions (‚â•0.8)
- `leads_warm`: Medium coherence (0.6-0.8)
- `leads_cold`: Low coherence (0.4-0.6)
- `leads_disqualified`: Failed scrapes (<0.4)
- `outcomes_won`: Successful crawls
- `outcomes_lost`: Failed/blocked crawls
- `deal_value`: Leads extracted √ó value per lead
- `total_cost`: LLM API costs

### ML Model Trials
- `leads_qualified`: Predictions made
- `leads_hot/warm/cold`: Confidence tiers
- `outcomes_won/lost`: Correct vs incorrect predictions
- `total_cost`: Inference costs

## Fitness Score Calculation

Each branch gets a fitness score:

```python
fitness = (
    qualification_rate * 0.3 +  # % not disqualified
    conversion_rate * 0.5 +      # % won of outcomes
    revenue_weight * 0.2         # Normalized avg deal value
)
```

Winner determined by:
- Experimental > Classic √ó 1.05 ‚Üí Experimental wins
- Classic > Experimental √ó 1.05 ‚Üí Classic wins
- Otherwise ‚Üí Inconclusive, need more data

Confidence based on:
- Sample size (larger = higher confidence)
- Difference magnitude (bigger gap = higher confidence)

## Directory Structure

```
trials/
‚îú‚îÄ‚îÄ README.md                      # This file
‚îú‚îÄ‚îÄ crawl4ai_hunter_trial.py      # Crawl4AI vs classic hunter
‚îú‚îÄ‚îÄ ml_scorer_trial.py            # Future: ML lead scoring trial
‚îî‚îÄ‚îÄ ai_agent_trial.py             # Future: AI agent qualification trial

data/trials/
‚îú‚îÄ‚îÄ trial_YYYYMMDD_HHMMSS.json    # Trial state
‚îú‚îÄ‚îÄ current_standard.json          # Current winning config
‚îî‚îÄ‚îÄ results/
    ‚îî‚îÄ‚îÄ trial_*_result.json        # Evaluation results
```

## CERATA Evolution Cycle

```
1. HUNT     ‚Üí Find new prey (external repositories)
2. EXTRACT  ‚Üí Clone nematocysts (useful patterns)
3. ADAPT    ‚Üí Modify for Rose Glass integration
4. TRIAL    ‚Üí Test experimental vs classic (YOU ARE HERE)
5. EVALUATE ‚Üí Measure fitness, determine winner
6. PROMOTE  ‚Üí Winner becomes new standard
7. GRAVEYARD‚Üí Loser feeds nutrients for next hunt
```

## Best Practices

1. **Always define clear success criteria** before starting trial
2. **Use 50/50 split** unless you have reason for different allocation
3. **Wait for min_sample_size** before evaluating (statistical validity)
4. **Track relevant metrics** for your trial type (map to TrialBranch fields)
5. **Document learnings** from both winners and losers
6. **Promote winners promptly** to avoid drift from standard
7. **Archive losers with rationale** to feed future experiments

## Future Trials

**Planned**:
- ML Lead Scoring (scikit-learn/XGBoost vs rule-based)
- AI Agent Qualification (LangChain agents vs manual)
- Multi-Modal Extraction (Vision + Text vs Text-only)
- Distributed Hunting (Celery workers vs single process)

**Ideas from Graveyard**:
- Weighted lens experiments
- Authority-heavy vs intent-heavy scoring
- Temporal decay models
- Graph-based lead relationships

---

üåπ **Rose Glass sees all, judges none, learns always**
